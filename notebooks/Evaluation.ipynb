{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "Wir beginnen mit dem Setup. Wir bauen eine kleine RAG Klasse, mit der wir einfacher arbeiten können.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "\n",
    "if os.getcwd().endswith(\"notebooks\"):\n",
    "    os.chdir(\"..\")\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class RAG:\n",
    "    def __init__(self, model=\"gpt-4.1-mini\"):\n",
    "        self.llm = AzureChatOpenAI(model=model)\n",
    "        self.embeddings = AzureOpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "        self.doc_embeddings = None\n",
    "        self.docs = None\n",
    "\n",
    "    def load_documents(self, documents):\n",
    "        \"\"\"Load documents and compute their embeddings.\"\"\"\n",
    "        self.docs = documents\n",
    "        self.doc_embeddings = self.embeddings.embed_documents(documents)\n",
    "\n",
    "    def get_most_relevant_docs(self, query):\n",
    "        \"\"\"Find the most relevant document for a given query.\"\"\"\n",
    "        if not self.docs or not self.doc_embeddings:\n",
    "            raise ValueError(\"Documents and their embeddings are not loaded.\")\n",
    "\n",
    "        query_embedding = self.embeddings.embed_query(query)\n",
    "        similarities = [\n",
    "            np.dot(query_embedding, doc_emb)\n",
    "            / (np.linalg.norm(query_embedding) * np.linalg.norm(doc_emb))\n",
    "            for doc_emb in self.doc_embeddings\n",
    "        ]\n",
    "        most_relevant_doc_index = np.argmax(similarities)\n",
    "        return [self.docs[most_relevant_doc_index]]\n",
    "\n",
    "    def generate_answer(self, query, relevant_doc):\n",
    "        \"\"\"Generate an answer for a given query based on the most relevant document.\"\"\"\n",
    "        prompt = f\"question: {query}\\n\\nDocuments: {relevant_doc}\"\n",
    "        messages = [\n",
    "            (\"system\", \"You are a helpful assistant that answers questions based on given documents only.\"),\n",
    "            (\"human\", prompt),\n",
    "        ]\n",
    "        ai_msg = self.llm.invoke(messages)\n",
    "        return ai_msg.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier sind eine Metriken die wir zur Evaluation verwenden:\n",
    "\n",
    "* Context-Recall: Anzahl vom Kontext untermauerter Aussagen / Anzahl Aussagen \n",
    "    * ein LLM bricht hierzu die Antwort in Aussagen auf und beurteilt für jede dieser, ob sie aus dem Kontxt hervorgeht\n",
    "* Faithfulness: misst die Qualität des generierten Textes hinsichtlich dquality of your RAG pipeline's generator by evaluating whether the actual_output factually aligns with the contents of your retrieval_context.\n",
    "\n",
    "For this we will create a test case below. We will make use of DeepEval, a framework for evaluation that provides many convenient measures.\n",
    "\n",
    "Before we can use it, we'll need to do some setup. Run the below lines on your terminal before continuing with the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir initialisieren eine Instanz der RAG Klasse und laden ein Dokument ein:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Initialize RAG instance\n",
    "rag = RAG()\n",
    "\n",
    "# Load documents\n",
    "pdf_path = \"data/Broschure-Sortiment-MicroFluidics.pdf\"\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "rag.load_documents(documents=\n",
    "    [p.page_content for p in loader.load_and_split(text_splitter=RecursiveCharacterTextSplitter())]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun sind wir bereit für eine Testanfrage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query and retrieve the most relevant document\n",
    "query = \"What is the whisper technology?\"\n",
    "relevant_doc = rag.get_most_relevant_docs(query)\n",
    "\n",
    "# Generate an answer\n",
    "answer = rag.generate_answer(query, relevant_doc)\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Relevant Document: {relevant_doc}\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe\n",
    "\n",
    "1.1. Welche Metriken gibt es? Siehe: https://docs.ragas.io/en/latest/concepts/metrics/available_metrics\n",
    "\n",
    "1.2. Vergleiche die Modelle \"gpt-4o\" und \"gpt-4.1-mini\" oder probiere andere Parameter des Textsplitters. Wie verändern sich die Metriken?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_queries = [\n",
    "    \"What is the whisper technology?\",\n",
    "]\n",
    "\n",
    "expected_responses = [\n",
    "   \"Whisper technology refers to an innovative valve drive technology used in Bürkert Whisper Valves.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "for query,reference in zip(sample_queries,expected_responses):\n",
    "\n",
    "    relevant_docs = rag.get_most_relevant_docs(query)\n",
    "    response = rag.generate_answer(query, relevant_docs)\n",
    "    dataset.append(\n",
    "        {\n",
    "            \"user_input\":query,\n",
    "            \"retrieved_contexts\":relevant_docs,\n",
    "            \"response\":response,\n",
    "            \"reference\":reference\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import EvaluationDataset\n",
    "evaluation_dataset = EvaluationDataset.from_list(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "\n",
    "llm = AzureChatOpenAI(model=\"gpt-4.1-mini\")\n",
    "embeddings = AzureOpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "evaluator_llm = LangchainLLMWrapper(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import Faithfulness, FactualCorrectness\n",
    "\n",
    "result = evaluate(\n",
    "    dataset=evaluation_dataset,\n",
    "    metrics=[Faithfulness(), FactualCorrectness(mode=\"precision\")],\n",
    "    llm=evaluator_llm\n",
    ")\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
