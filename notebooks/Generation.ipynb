{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wie generieren wir Texte mit LLMs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier nutzen wir die OpenAI API, um Anfragen zu senden und LLM (GPT 4.1-mini) Antworten von OpenAI zu erhalten.\n",
    "Wir verwenden die Langchain-Klasse AzureOpenAI, um diese Interaktion zu verwalten. Der Prozess ist dann äußerst einfach:\n",
    "\n",
    "Wir generieren eine Input-Prompt und verwenden die \"invoke\" Methode der AzureOpenAI Klasse.\n",
    "\n",
    "Beachten Sie, dass jedes LLM eine begrenzte Kontextlänge hat, die den maximalen Input und Output begrenzt, den es verarbeiten kann. Wenn der generierte Output also zu lang wird, wird die Antwort abgeschnitten. Dies ist ein allgemeines Problem, das auf viele Arten angegangen werden kann - die einfachste davon ist der Versuch, Prompt Engineering zu nutzen, um (die meisten) dieser Fälle zu verhindern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display  # for displaying markdown\n",
    "from langchain_openai import AzureChatOpenAI   # get the AzureOpenAI class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chat_generator(model_name = \"gpt-4.1-mini\"):\n",
    "    return AzureChatOpenAI(model=model_name, temperature=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prompt = \"Write someting.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_generator = get_chat_generator(\"gpt-4.1-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat_generator.invoke(input_prompt)\n",
    "display(Markdown(response.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bis jetzt machen wir keine \"retrieval augmented generation\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prompt = \"Explain the what the Whisper technology by Bürkert Fluid Control Systems GmbH is. If you don't know the answer, say that you don't know.\"\n",
    "response = chat_generator.invoke(input_prompt)\n",
    "display(Markdown(response.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aufgabe 1.1: Vergleichen Sie 4.1-mini und das größere 4-o-Modell - sehen Sie einen Unterschied?\n",
    "\n",
    "Aufgabe 1.2: Wenn alles in der obigen Zelle funktioniert, sollte das Modell Ihnen mitteilen, dass es es nicht weiß. Wissen Sie warum?\n",
    "\n",
    "Aufgabe 1.3: Wie können Sie den Prompt ändern, um das Modell zum Halluzinieren zu bringen? Welche anderen Möglichkeiten fallen Ihnen ein, um das Auftreten einer Halluzination in einer Antwort zu verhindern oder anzuzeigen?\n",
    "\n",
    "Aufgabe 1.4 (etwas fortgeschrittener): Wie können Sie die obige Generierung Retrieval-augmentiert gestalten? Denken Sie an eine super einfache und manuelle Methode.\n",
    "\n",
    "Aufgabe 1.5 (falls Sie sich langweilen): Können Sie den Vektorstore aus dem vorherigen Notebook verwenden, um ein einfaches RAG-System unter Verwendung der obigen Funktionen zu implementieren?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
