{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wie generieren wir Texte mit LLMs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier nutzen wir die OpenAI API, um Anfragen zu senden und LLM (GPT 4.1-mini) Antworten von OpenAI zu erhalten.\n",
    "Wir verwenden die Langchain-Klasse AzureOpenAI, um diese Interaktion zu verwalten. Der Prozess ist dann äußerst einfach:\n",
    "\n",
    "Wir generieren eine Input-Prompt und verwenden die \"invoke\" Methode der AzureOpenAI Klasse.\n",
    "\n",
    "Beachten Sie, dass jedes LLM eine begrenzte Kontextlänge hat, die den maximalen Input und Output begrenzt, den es verarbeiten kann. Wenn der generierte Output also zu lang wird, wird die Antwort abgeschnitten. Dies ist ein allgemeines Problem, das auf viele Arten angegangen werden kann - die einfachste davon ist der Versuch, Prompt Engineering zu nutzen, um (die meisten) dieser Fälle zu verhindern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display  # for displaying markdown\n",
    "from langchain_openai import AzureChatOpenAI   # get the AzureOpenAI class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chat_generator(model_name = \"gpt-4.1-mini\"):\n",
    "    return AzureChatOpenAI(model=model_name, temperature=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prompt = \"Write someting.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_generator = get_chat_generator(\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "As of my knowledge cutoff in October 2023, I do not have specific information about a technology called \"Whisper\" by Bürkert GmbH. Bürkert is a company known for its expertise in fluid control systems, including valves, sensors, and controllers, but I cannot confirm the existence or details of a \"Whisper\" technology from them. If this is a recent development or a niche product, I recommend consulting Bürkert's official website or contacting them directly for accurate information. \n",
       "\n",
       "(Note: This response does not involve hallucination.)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = chat_generator.invoke(input_prompt)\n",
    "display(Markdown(response.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bis jetzt machen wir keine \"retrieval augmented generation\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "As of my knowledge cutoff in October 2023, Bürkert GmbH, a company specializing in fluid control systems, has developed a technology called \"Whisper\" that is associated with their solenoid valve systems. Whisper technology is designed to minimize noise and vibration during valve operation, making it particularly suitable for applications where quiet operation is critical, such as in medical devices, laboratory equipment, or other sensitive environments. The technology focuses on reducing the sound generated by the valve's actuation, ensuring smooth and silent performance.\n",
       "\n",
       "I am reasonably confident in this explanation, as it aligns with Bürkert's known expertise in fluid control and their focus on innovative solutions for precision and noise reduction. However, for the most accurate and up-to-date information, I recommend consulting Bürkert's official documentation or contacting the company directly."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_prompt = \"\"\"\n",
    "<context>\n",
    "source: Broschure-Sortiment-MicroFluidics.pdf\n",
    "content: Whisper-Technologie\n",
    "Noch nie ließen sich Magnetventile so  \n",
    "genau steuern. Noch nie waren sie so schnell.  \n",
    "Noch nie konnten in einem solchen Umfang \n",
    "Daten aus dem Ventil analysiert werden.  \n",
    "Mit dem innovativen Whisper-Antrieb ist all \n",
    "das möglich. Hochpräzise Ventilsteuerung  \n",
    "und Diagnosefunktionen ermöglichen Ihnen \n",
    "viele neue Funktionen in der Anwendung.\n",
    "</context>\n",
    "Explain what the Whisper technology by Bürkert GmbH is. If you don't know, say 'I don't know'.\"\n",
    "response = chat_generator.invoke(input_prompt)\n",
    "display(Markdown(response.content))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aufgabe 1.1: Vergleichen Sie 4.1-mini und das größere 4-o-Modell - sehen Sie einen Unterschied?\n",
    "\n",
    "Aufgabe 1.2: Wenn alles in der obigen Zelle funktioniert, sollte das Modell Ihnen mitteilen, dass es es nicht weiß. Wissen Sie warum?\n",
    "\n",
    "Aufgabe 1.3: Wie können Sie den Prompt ändern, um das Modell zum Halluzinieren zu bringen? Welche anderen Möglichkeiten fallen Ihnen ein, um das Auftreten einer Halluzination in einer Antwort zu verhindern oder anzuzeigen?\n",
    "\n",
    "Aufgabe 1.4 (etwas fortgeschrittener): Wie können Sie die obige Generierung Retrieval-augmentiert gestalten? Denken Sie an eine super einfache und manuelle Methode.\n",
    "\n",
    "Aufgabe 1.5 (falls Sie sich langweilen): Können Sie den Vektorstore aus dem vorherigen Notebook verwenden, um ein einfaches RAG-System unter Verwendung der obigen Funktionen zu implementieren?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
