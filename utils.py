from collections.abc import Callable
from dataclasses import dataclass
from pathlib import Path
import pdb
from typing import Literal

import streamlit as st
from dotenv import load_dotenv
from langchain_community.document_loaders import PyPDFLoader
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.vectorstores import VectorStoreRetriever
from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings
from langchain_qdrant import QdrantVectorStore
from langchain_text_splitters import RecursiveCharacterTextSplitter
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams

load_dotenv()


def get_chat_generator(
    model_name: Literal["gpt-4.1-mini", "gpt-4o"] = "gpt-4.1-mini",
) -> AzureChatOpenAI:
    return AzureChatOpenAI(model=model_name, temperature=0.0)


def get_embedder() -> AzureOpenAIEmbeddings:
    return AzureOpenAIEmbeddings(model="text-embedding-3-small")

pdb.set_trace()

@st.cache_resource(show_spinner="Loading document...")
def get_pdf_text(pdf_path: Path) -> str:
    """
    Creates and returns the content of the PDF documents.
    This function is cached to avoid re-processing the document on every run.
    """

    # Load the document and combine the contents of each page
    loader = PyPDFLoader(pdf_path)
    pdf_pages = loader.load()
    pdf_text = ""
    for page in pdf_pages:
        pdf_text += page.page_content

    return pdf_text


@st.cache_resource(show_spinner="Indexing document...")
def get_retriever(pdf_paths: list[Path]) -> VectorStoreRetriever:
    """
    Creates and returns a retriever from the PDF document.
    This function is cached to avoid re-processing the document on every run.
    """

    # Initialize the qdrant client
    qdrant_client = QdrantClient(":memory:")

    if not qdrant_client.collection_exists("rag_chat"):
        qdrant_client.create_collection(
            collection_name="rag_chat",
            vectors_config=VectorParams(size=1536, distance=Distance.COSINE),
        )

    # 2. Setup the vectorstore
    vectorstore = QdrantVectorStore(
        client=qdrant_client,
        collection_name="rag_chat",
        embedding=get_embedder(),
    )

    #
    # Aufgabe 1.: Lies die PDF files ein, chunke diese und füge sie dem vectorstore
    #          hinzu.
    #
    # Vorgehen:
    #   * 1.1. PDF einlesen und splitten
    #   * 1.2. Siehe: https://docs.langchain.com/oss/python/integrations/splitters/index#text-splitters
    #   * 1.3. Experimentiere mit den Parametern des Text-splitters
    #   * 1.4. Speichere die chunks im vectorstore
    #
    # Tipps:
    #   * Siehe: https://docs.langchain.com/oss/python/langchain/overview

    #
    # Lösung:
    #
    # Load the document
    for pdf_path in pdf_paths:
        loader = PyPDFLoader(pdf_path)
        # 3. Split the document into chunks
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000, chunk_overlap=200
        )
        chunks = loader.load_and_split(text_splitter=text_splitter)
        # 4. Add the chunks to the vector store
        vectorstore.add_documents(documents=chunks)
    #
    # /Lösung
    #

    retriever = vectorstore.as_retriever(
        search_type="mmr", search_kwargs={"k": 2, "lambda_mult": 0.25}
    )

    return retriever


@dataclass
class RAGSource:
    """
    Dataclass for holding info about the sources used by the RAG
    system for the response.
    Contains:
        file: the name of the source file containing the retrieved chunk
        page: the page number of the retrieved chunk
        text: the content of the retrieved chunk
    """

    file: str
    page: str
    text: str


@dataclass
class RAGResponse:
    """
    Dataclass for holding the response of the RAG system.
    Contains:
        answer: the answer generated by the LLM
        sources: info about the chunks used as context by the LLM
    """

    answer: str
    sources: list[RAGSource]


@st.cache_resource(show_spinner=True)
def get_response_generator() -> Callable:
    """
    Creates and returns a function for answering questions.
    """

    def get_response(query: str, retriever: VectorStoreRetriever, messages) -> RAGResponse:
        #
        # Aufgabe 2.: Implementiere den RAG workflow:
        #
        # Vorgehen:
        #   * 2.1. Prompt definieren. Wichtig: placeholder 'query' und 'context' verwenden
        #   * 2.2. Addiere den Inhalt der Chunks zum Kontext 'context'.
        #   * 2.3. Speichere die Metadaten der Chunks als Quellenverweise in 'sources'.
        #   * 2.4. Rufe das PromptTemplate mit den konkreten Variablen auf.
        #   * 2.5. Verwende das Resultat als input für das LLM.
        #   * 2.6. Befülle den Wiedergabewert 'RAGResponse'.
        #
        # Tipps:
        #   * Siehe: https://reference.langchain.com/python/langchain/

        prompt = ChatPromptTemplate.from_template("""
        You are a helpful assitant. Use the context to answer the user query.                         
        {context}
        Question: {query}
        """)

        retrieved_docs = retriever.invoke(query)
        context: str = "<context>"
        sources: list[RAGSource] = []
        for doc in retrieved_docs:
            context += doc.page_content
        context += "</context>"

        # Get the model
        llm = get_chat_generator()
        # ...DEIN CODE HIER...
        content = "I am not implemented yet."

        #
        # Lösung:
        #

        prompt = ChatPromptTemplate(
            [
                ("system", "{context}"),
                ("placeholder", "{conversation}"),
                ("user", "{input}")
            ]
        )

        retrieved_docs = retriever.invoke(query)
        context: str = "<context>"
        sources = []
        for doc in retrieved_docs:
            context += doc.page_content
            context += "\n\n"
            sources.append(
                RAGSource(
                    doc.metadata["source"],
                    doc.metadata["page_label"],
                    doc.page_content,
                )
            )
        context += "</context>"

        # Get the model
        llm = get_chat_generator()
        # Generate a response
        chain = prompt | llm
        response = chain.invoke({"input": query, "context": context, "conversation": messages})
        # Handle the response content properly
        content = str(response.content)
        #
        # /Lösung
        #
        return RAGResponse(content, sources)

    return get_response
